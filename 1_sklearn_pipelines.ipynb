{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From classical Sklearn ML workflow to Sklearn pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib notebook\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_with_sampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Machine learning pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orignal Authors: Jesse E.Agbe (JCharis)\n",
    "# Original source: https://blog.jcharistech.com/2021/02/05/building-machine-learning-pipelines-with-scikit-learn-python/#:~:text=A%20Pipeline%20consists%20of%20a,means%20of%20automating%20a%20workflow.\n",
    "# Modified by Dr Adnane Ez-zizi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. What is a machine learning pipeline?\n",
    "\n",
    "A Machine Learning (ML) pipeline as a sequence of processing elements or functions, where the output of one element becomes the input for the next. It is a method for chaining functions and tasks typically found in workflows and is used across various fields such as Data Science, Machine Learning, DevOps, Manufacturing, and general Software Development. The concept mirrors the continuous life cycle of an assembly line in the manufacturing industry.\n",
    "\n",
    "ML Pipeline serve to automate the Machine Learning workflow, enabling the codification and automation of producing usable ML models. It is an independently executable workflow that completes an ML task by executing tasks in sequence automatically, including data transformation, training, and model building, to achieve a specific output. This automation aims to package workflows or sequences of tasks to enhance efficiency and organization, ensuring the process is well-structured and reproducible.\n",
    "\n",
    "### 1.2. Advantages of using ML pipelines\n",
    "\n",
    "- Making the building of models more efficient and simplified.\n",
    "- Helping to cut redundant work.\n",
    "- Moving the product from just the model to a complete pipeline/workflow, which improves efficiency and scalability.\n",
    "- Making it easier to monitor and tune each component of the process.\n",
    "- Reducing the chance of error and saving time by automating repetitive tasks.\n",
    "\n",
    "### 1.3. Pipeline stages\n",
    "\n",
    "1) **Transformer Stage:** A transformer takes a dataset as input and produces a transformed/augmented dataset as output. It processes the data and converts it into a feature-ready dataset. An example of this is a tokenizer.\n",
    "\n",
    "2) **Estimator Stage:** An estimator is fitted on an input dataset and produces a model that can be used to perform predictive tasks. Examples include Naive Bayes and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Build a Simple ML Pipeline with Scikit-Learn\n",
    "\n",
    "First, define the pipeline by specifying the sequence of the pipeline steps. Each step is a tuple consisting of a name and an instance of a transformer or an estimator. For a Logistic Regression pipeline, you might include a scaler (e.g. `StandardScaler`) followed by the logistic regression estimator (`LogisticRegression`). The code looks like this:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create the pipeline\n",
    "pipe_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "```\n",
    "\n",
    "Next, we fit the pipeline to our training data. This step will execute the scaler transformation followed by training the logistic regression model:\n",
    "\n",
    "```python\n",
    "# Fit the pipeline to the training data\n",
    "pipe_lr = pipe_lr.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "After fitting the model, you can proceed with making predictions, evaluating the model, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Example\n",
    "\n",
    "This example was used in Session 7 to illustrate the problem induced by learning on datasets having imbalanced classes. Here we will use it to show how to use Sklearn pipelines to make the process of building machine learning pipelines easier and more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orignal Author: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n",
    "# License: MIT\n",
    "# Original source: https://imbalanced-learn.org/stable/auto_examples/applications/plot_impact_imbalanced_classes.html#sphx-glr-auto-examples-applications-plot-impact-imbalanced-classes-py\n",
    "# Modified by Dr Adnane Ez-zizi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We are using a modified version of the \"adult\" (income) dataset from sklearn.datasets without dropping features:\n",
    "\n",
    "- \"fnlwgt\": this feature was created while studying the \"adult\" dataset. Thus, we will not use this feature which is not acquired during the survey.\n",
    "- \"education-num\": it is encoding the same information than \"education\". Thus, we are removing one of these 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>10th</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age workclass     education      marital-status         occupation  \\\n",
       "0  25.0   Private          11th       Never-married  Machine-op-inspct   \n",
       "1  38.0   Private       HS-grad  Married-civ-spouse    Farming-fishing   \n",
       "2  18.0       NaN  Some-college       Never-married                NaN   \n",
       "3  34.0   Private          10th       Never-married      Other-service   \n",
       "4  29.0       NaN       HS-grad       Never-married                NaN   \n",
       "\n",
       "    relationship   race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0      Own-child  Black    Male           0.0           0.0            40.0   \n",
       "1        Husband  White    Male           0.0           0.0            50.0   \n",
       "2      Own-child  White  Female           0.0           0.0            30.0   \n",
       "3  Not-in-family  White    Male           0.0           0.0            30.0   \n",
       "4      Unmarried  Black    Male           0.0           0.0            40.0   \n",
       "\n",
       "  native-country  class  \n",
       "0  United-States  <=50K  \n",
       "1  United-States  <=50K  \n",
       "2  United-States  <=50K  \n",
       "3  United-States  <=50K  \n",
       "4  United-States  <=50K  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Adult_income.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38393 entries, 0 to 38392\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             38393 non-null  float64\n",
      " 1   workclass       35829 non-null  object \n",
      " 2   education       38393 non-null  object \n",
      " 3   marital-status  38393 non-null  object \n",
      " 4   occupation      35819 non-null  object \n",
      " 5   relationship    38393 non-null  object \n",
      " 6   race            38393 non-null  object \n",
      " 7   sex             38393 non-null  object \n",
      " 8   capital-gain    38393 non-null  float64\n",
      " 9   capital-loss    38393 non-null  float64\n",
      " 10  hours-per-week  38393 non-null  float64\n",
      " 11  native-country  37735 non-null  object \n",
      " 12  class           38393 non-null  object \n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Description of the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical columns to category type\n",
    "df = df.astype({\"workclass\":\"category\", \n",
    "                \"education\":\"category\", \n",
    "                \"marital-status\":\"category\", \n",
    "                \"occupation\":\"category\", \n",
    "                \"relationship\":\"category\", \n",
    "                \"race\":\"category\", \n",
    "                \"sex\":\"category\", \n",
    "                \"native-country\":\"category\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38393 entries, 0 to 38392\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   age             38393 non-null  float64 \n",
      " 1   workclass       35829 non-null  category\n",
      " 2   education       38393 non-null  category\n",
      " 3   marital-status  38393 non-null  category\n",
      " 4   occupation      35819 non-null  category\n",
      " 5   relationship    38393 non-null  category\n",
      " 6   race            38393 non-null  category\n",
      " 7   sex             38393 non-null  category\n",
      " 8   capital-gain    38393 non-null  float64 \n",
      " 9   capital-loss    38393 non-null  float64 \n",
      " 10  hours-per-week  38393 non-null  float64 \n",
      " 11  native-country  37735 non-null  category\n",
      " 12  class           38393 non-null  object  \n",
      "dtypes: category(8), float64(4), object(1)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     age  capital-gain  capital-loss  hours-per-week\n",
      "age             1.000000      0.078726      0.038444        0.148751\n",
      "capital-gain    0.078726      1.000000     -0.040948        0.032147\n",
      "capital-loss    0.038444     -0.040948      1.000000        0.036330\n",
      "hours-per-week  0.148751      0.032147      0.036330        1.000000\n"
     ]
    }
   ],
   "source": [
    "corr_matrix = df.corr(method='spearman')\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate the predictors and response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'class')\n",
    "Y = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has a class ratio of 30:1 in favour of the class <=50K, so very imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<=50K    37155\n",
       ">50K      1238\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_count = Y.value_counts()\n",
    "classes_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to the modelling, let's recode the target variable into 0 and 1 (i.e. '>50K': 1, '<=50K': 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode the target variable to 0 and 1\n",
    "Y = Y.map({'>50K': 1, '<=50K': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies to learn from an imbalanced dataset\n",
    "We will use a dictionary and a list to continuously store the results of our experiments and show them as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "scores = {\"Accuracy\": [], \"Balanced accuracy\": [], \"F1-score\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a cross-validation evaluation to get an estimate of the test accuracy score. We will using both the standard accuracy and the balanced accuracy, which is the average the accuracy over both classes.\n",
    "\n",
    "As a baseline, we could use a classifier which will always predict the majority class independently of the features provided. This is what we call the dummy baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy baseline\n",
    "\n",
    "Before to train a real machine learning model, we can store the results\n",
    "obtained with our :class:`~sklearn.dummy.DummyClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy classifier</th>\n",
       "      <td>0.967755</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.491807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Accuracy  Balanced accuracy  F1-score\n",
       "Dummy classifier  0.967755                0.5  0.491807"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.append(\"Dummy classifier\")\n",
    "scoring = [\"accuracy\", \"balanced_accuracy\", \"f1_macro\"]\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "cv_result = cross_validate(dummy_clf, X, Y, scoring = scoring)\n",
    "scores[\"Accuracy\"].append(cv_result[\"test_accuracy\"].mean())\n",
    "scores[\"Balanced accuracy\"].append(cv_result[\"test_balanced_accuracy\"].mean())\n",
    "scores[\"F1-score\"].append(cv_result[\"test_f1_macro\"].mean())  \n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear classifier baseline\n",
    "\n",
    "We will create a machine learning pipeline using a `sklearn.linear_model.LogisticRegression` classifier and 5-fold cross-validation. As part of this pipline, we will need to one-hot encode the categorical columns and standardized the numerical columns before to inject the data into the `sklearn.linear_model.LogisticRegression` classifier.\n",
    "\n",
    "First, we define our numerical and categorical pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy classifier</th>\n",
       "      <td>0.967755</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.491807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic regression</th>\n",
       "      <td>0.970620</td>\n",
       "      <td>0.574108</td>\n",
       "      <td>0.616515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy  Balanced accuracy  F1-score\n",
       "Dummy classifier     0.967755           0.500000  0.491807\n",
       "Logistic regression  0.970620           0.574108  0.616515"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Without using Sklearn pipelines ################\n",
    "\n",
    "# Identifying continuous and categorical columns\n",
    "continuous_columns = X.select_dtypes(include=\"number\").columns\n",
    "categorical_columns = X.select_dtypes(exclude=\"number\").columns\n",
    "\n",
    "# Define Stratified K-Fold cross-validation \n",
    "# (stratified to make sure that we retain the same original proportion of classes in each fold)\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Initialise lists to store the results for each fold from the cross-validation\n",
    "acc_scores = []\n",
    "bal_acc_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, Y):\n",
    "    \n",
    "    ### Splitting the data into training and test sets for this fold\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "    ### Preprocessing continuous columns for this fold\n",
    "    # Step 1: Impute missing values for continuous columns with their mean\n",
    "    imputer_cont = SimpleImputer(strategy='mean')\n",
    "    X_train_cont_imputed = imputer_cont.fit_transform(X_train[continuous_columns])\n",
    "    # Step 2: Scale the continuous columns\n",
    "    scaler = StandardScaler()\n",
    "    X_train_cont_scaled = scaler.fit_transform(X_train_cont_imputed)\n",
    "    # Step 3: Apply the same transformation to test data\n",
    "    X_test_cont_imputed = imputer_cont.transform(X_test[continuous_columns])\n",
    "    X_test_cont_scaled = scaler.transform(X_test_cont_imputed)\n",
    "\n",
    "    ### Preprocessing categorical columns for this fold\n",
    "    # Step 1: Impute missing values for categorical columns with the value 'missing'\n",
    "    imputer_cat = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "    X_train_cat_imputed = imputer_cat.fit_transform(X_train[categorical_columns])\n",
    "    # Step 2: Transform categorical columns using OneHotEncoder\n",
    "    encoder_cat = OneHotEncoder(handle_unknown='ignore')\n",
    "    X_train_cat_encoded = encoder_cat.fit_transform(X_train_cat_imputed)\n",
    "    # Step 3: Apply the same transformation to test data\n",
    "    X_test_cat_imputed = imputer_cat.transform(X_test[categorical_columns])\n",
    "    X_test_cat_encoded = encoder_cat.transform(X_test_cat_imputed)\n",
    "\n",
    "    ### Combine continuous and categorical preprocessed columns\n",
    "    X_train_preprocessed = sparse.hstack((X_train_cont_scaled, X_train_cat_encoded))\n",
    "    X_test_preprocessed = sparse.hstack((X_test_cont_scaled, X_test_cat_encoded))\n",
    "\n",
    "    ### Fit Logistic Regression model\n",
    "    lr_clf = LogisticRegression(max_iter=500)\n",
    "    #lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train_preprocessed, Y_train)\n",
    "\n",
    "    ### Predict and evaluate on the test set\n",
    "    Y_pred = lr_clf.predict(X_test_preprocessed)\n",
    "    acc_scores.append(accuracy_score(Y_test, Y_pred))\n",
    "    bal_acc_scores.append(balanced_accuracy_score(Y_test, Y_pred))\n",
    "    f1_scores.append(f1_score(Y_test, Y_pred, average='macro'))  \n",
    "\n",
    "# Calculating mean scores across all folds\n",
    "mean_accuracy = np.mean(acc_scores)\n",
    "mean_balanced_accuracy = np.mean(bal_acc_scores)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "# Append the results to scores and df_scores\n",
    "index += [\"Logistic regression\"]\n",
    "scores[\"Accuracy\"].append(mean_accuracy)\n",
    "scores[\"Balanced accuracy\"].append(mean_balanced_accuracy)\n",
    "scores[\"F1-score\"].append(mean_f1)\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our linear model is learning slightly better than our dummy baseline. However, it is impacted by the class imbalance. \n",
    "Now let's re-run the same machine learning pipeline using Sklearn.\n",
    "\n",
    "First, we define our numerical and categorical pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### With Sklearn pipelines ################\n",
    "\n",
    "num_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy=\"mean\"), StandardScaler() \n",
    ")\n",
    "\n",
    "cat_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can create a preprocessor which will dispatch the categorical\n",
    "columns to the categorical pipeline and the numerical columns to the\n",
    "numerical pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_linear = make_column_transformer(\n",
    "    (num_pipe, selector(dtype_include=\"number\")),\n",
    "    (cat_pipe, selector(dtype_include=\"category\")),\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we connect our preprocessor with our `sklearn.linear_model.LogisticRegression`. We can then evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = make_pipeline(preprocessor_linear, LogisticRegression(max_iter=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy classifier</th>\n",
       "      <td>0.967755</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.491807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic regression</th>\n",
       "      <td>0.970620</td>\n",
       "      <td>0.574108</td>\n",
       "      <td>0.616515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic regression with sklearn pipeline</th>\n",
       "      <td>0.970620</td>\n",
       "      <td>0.574108</td>\n",
       "      <td>0.616515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Accuracy  Balanced accuracy  \\\n",
       "Dummy classifier                           0.967755           0.500000   \n",
       "Logistic regression                        0.970620           0.574108   \n",
       "Logistic regression with sklearn pipeline  0.970620           0.574108   \n",
       "\n",
       "                                           F1-score  \n",
       "Dummy classifier                           0.491807  \n",
       "Logistic regression                        0.616515  \n",
       "Logistic regression with sklearn pipeline  0.616515  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Stratified K-Fold cross-validation \n",
    "# (stratified to make sure that we retain the same original proportion of classes in each fold)\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "index += [\"Logistic regression with sklearn pipeline\"]\n",
    "cv_result = cross_validate(lr_clf, X, Y, scoring=scoring, cv=skf)\n",
    "scores[\"Accuracy\"].append(cv_result[\"test_accuracy\"].mean())\n",
    "scores[\"Balanced accuracy\"].append(cv_result[\"test_balanced_accuracy\"].mean())\n",
    "scores[\"F1-score\"].append(cv_result[\"test_f1_macro\"].mean())\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will present an approach to improve the performance by using under-sampling and over-sampling.\n",
    "\n",
    "### Resample the training set during learning\n",
    "\n",
    "One way to overcome class imbalance is to resample the training set by under-sampling or over-sampling some of the samples. `imbalanced-learn` provides some samplers to do such processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a random over-sampler before the training of the linear model or random forest allows us to not focus on the majority class at the cost of making more mistake for samples in the majority class (i.e. decreased accuracy).\n",
    "\n",
    "We could apply any type of samplers and find which sampler is working best on the current dataset. What about over-sampling now? Could you do some research on it and implement it (see https://imbalanced-learn.org/dev/references/over_sampling.html)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Without using Sklearn pipelines ################\n",
    "\n",
    "# Initialise over-sampler\n",
    "over_sampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Reinitialise lists to store the results for each fold\n",
    "acc_scores_os = []\n",
    "bal_acc_scores_os = []\n",
    "f1_scores_os = []\n",
    "\n",
    "# Define Stratified K-Fold cross-validation \n",
    "# (stratified to make sure that we retain the same original proportion of classes in each fold)\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in skf.split(X, Y):\n",
    "    \n",
    "    ### Splitting the data into training and test sets for this fold\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "    ### Apply over-sampling to the training data\n",
    "    X_train, Y_train = over_sampler.fit_resample(X_train, Y_train)\n",
    "\n",
    "    ### Preprocessing continuous columns for this fold\n",
    "    # Step 1: Impute missing values for continuous columns with their mean\n",
    "    imputer_cont = SimpleImputer(strategy='mean')\n",
    "    X_train_cont_imputed = imputer_cont.fit_transform(X_train[continuous_columns])\n",
    "    # Step 2: Scale the continuous columns\n",
    "    scaler = StandardScaler()\n",
    "    X_train_cont_scaled = scaler.fit_transform(X_train_cont_imputed)\n",
    "    # Step 3: Apply the same transformation to test data\n",
    "    X_test_cont_imputed = imputer_cont.transform(X_test[continuous_columns])\n",
    "    X_test_cont_scaled = scaler.transform(X_test_cont_imputed)\n",
    "\n",
    "    ### Preprocessing categorical columns for this fold\n",
    "    # Step 1: Impute missing values for categorical columns with the value 'missing'\n",
    "    imputer_cat = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "    X_train_cat_imputed = imputer_cat.fit_transform(X_train[categorical_columns])\n",
    "    # Step 2: Transform categorical columns using OneHotEncoder\n",
    "    encoder_cat = OneHotEncoder(handle_unknown='ignore')\n",
    "    X_train_cat_encoded = encoder_cat.fit_transform(X_train_cat_imputed)\n",
    "    # Step 3: Apply the same transformation to test data\n",
    "    X_test_cat_imputed = imputer_cat.transform(X_test[categorical_columns])\n",
    "    X_test_cat_encoded = encoder_cat.transform(X_test_cat_imputed)\n",
    "\n",
    "    ### Combine continuous and categorical preprocessed columns\n",
    "    X_train_preprocessed = sparse.hstack((X_train_cont_scaled, X_train_cat_encoded))\n",
    "    X_test_preprocessed = sparse.hstack((X_test_cont_scaled, X_test_cat_encoded))\n",
    "\n",
    "    ### Fit Logistic Regression model\n",
    "    lr_clf = LogisticRegression(max_iter=500)\n",
    "    lr_clf.fit(X_train_preprocessed, Y_train)\n",
    "\n",
    "    ### Predict and evaluate on the test set\n",
    "    Y_pred = lr_clf.predict(X_test_preprocessed)\n",
    "    acc_scores_os.append(accuracy_score(Y_test, Y_pred))\n",
    "    bal_acc_scores_os.append(balanced_accuracy_score(Y_test, Y_pred))\n",
    "    f1_scores_os.append(f1_score(Y_test, Y_pred, average='macro'))\n",
    "\n",
    "# Calculating mean scores across all folds\n",
    "mean_accuracy_oversampling = np.mean(acc_scores_os)\n",
    "mean_balanced_accuracy_oversampling = np.mean(bal_acc_scores_os)\n",
    "mean_f1_oversampling = np.mean(f1_scores_os)\n",
    "\n",
    "# Append the results to scores and df_scores\n",
    "index.append(\"Over-sampling Logistic Regression\")\n",
    "scores[\"Accuracy\"].append(mean_accuracy_oversampling)\n",
    "scores[\"Balanced accuracy\"].append(mean_balanced_accuracy_oversampling)\n",
    "scores[\"F1-score\"].append(mean_f1_oversampling)\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy classifier</th>\n",
       "      <td>0.967755</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.491807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic regression</th>\n",
       "      <td>0.970620</td>\n",
       "      <td>0.574108</td>\n",
       "      <td>0.616515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic regression with sklearn pipeline</th>\n",
       "      <td>0.970620</td>\n",
       "      <td>0.574108</td>\n",
       "      <td>0.616515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Over-sampling Logistic Regression with sklearn pipeline</th>\n",
       "      <td>0.811607</td>\n",
       "      <td>0.814417</td>\n",
       "      <td>0.555770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Accuracy  \\\n",
       "Dummy classifier                                    0.967755   \n",
       "Logistic regression                                 0.970620   \n",
       "Logistic regression with sklearn pipeline           0.970620   \n",
       "Over-sampling Logistic Regression with sklearn ...  0.811607   \n",
       "\n",
       "                                                    Balanced accuracy  \\\n",
       "Dummy classifier                                             0.500000   \n",
       "Logistic regression                                          0.574108   \n",
       "Logistic regression with sklearn pipeline                    0.574108   \n",
       "Over-sampling Logistic Regression with sklearn ...           0.814417   \n",
       "\n",
       "                                                    F1-score  \n",
       "Dummy classifier                                    0.491807  \n",
       "Logistic regression                                 0.616515  \n",
       "Logistic regression with sklearn pipeline           0.616515  \n",
       "Over-sampling Logistic Regression with sklearn ...  0.555770  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### With Sklearn pipelines ################\n",
    "\n",
    "num_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy=\"mean\"), StandardScaler() \n",
    ")\n",
    "\n",
    "cat_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\")\n",
    ")\n",
    "\n",
    "preprocessor_linear = make_column_transformer(\n",
    "    (num_pipe, selector(dtype_include=\"number\")),\n",
    "    (cat_pipe, selector(dtype_include=\"category\")),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr_clf_over = make_pipeline_with_sampler(\n",
    "    RandomOverSampler(random_state=42),\n",
    "    preprocessor_linear,\n",
    "    LogisticRegression(max_iter=500)\n",
    ")\n",
    "\n",
    "# Define Stratified K-Fold cross-validation \n",
    "# (stratified to make sure that we retain the same original proportion of classes in each fold)\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "index.append(\"Over-sampling Logistic Regression with sklearn pipeline\")\n",
    "cv_result = cross_validate(lr_clf_over, X, Y, scoring=scoring, cv=skf)\n",
    "scores[\"Accuracy\"].append(cv_result[\"test_accuracy\"].mean())\n",
    "scores[\"Balanced accuracy\"].append(cv_result[\"test_balanced_accuracy\"].mean())\n",
    "scores[\"F1-score\"].append(cv_result[\"test_f1_macro\"].mean())\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. EXERCISE\n",
    "\n",
    "1) Run a pipeline with over-sampling and a random forest classifier.\n",
    "2) Compare with random forest with under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (7, 3), indices imply (9, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   1701\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_form_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1702\u001b[1;33m         \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1703\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (7, 3), indices imply (9, 3)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-cc1a4b8d1a27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"F1-score\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"test_f1_macro\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mdf_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0mdf_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         ]\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   1704\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1706\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (7, 3), indices imply (9, 3)"
     ]
    }
   ],
   "source": [
    "# 1) Run a pipeline with over-sampling and a random forest classifier.\n",
    "# TODO: replace the content of this cell with your solution\n",
    "########### With Sklearn pipelines ################\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "num_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy=\"mean\"), StandardScaler() \n",
    ")\n",
    "\n",
    "cat_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\")\n",
    ")\n",
    "\n",
    "preprocessor_linear = make_column_transformer(\n",
    "    (num_pipe, selector(dtype_include=\"number\")),\n",
    "    (cat_pipe, selector(dtype_include=\"category\")),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rfc_clf_over = make_pipeline_with_sampler(\n",
    "    RandomOverSampler(random_state=42),\n",
    "    preprocessor_linear,\n",
    "    RandomForestClassifier(n_estimators=100)\n",
    ")\n",
    "\n",
    "# Define Stratified K-Fold cross-validation \n",
    "# (stratified to make sure that we retain the same original proportion of classes in each fold)\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "index.append(\"Over-sampling Random Forest Classifier with sklearn pipeline\")\n",
    "cv_result = cross_validate(rfc_clf_over, X, Y, scoring=scoring, cv=skf)\n",
    "scores[\"Accuracy\"].append(cv_result[\"test_accuracy\"].mean())\n",
    "scores[\"Balanced accuracy\"].append(cv_result[\"test_balanced_accuracy\"].mean())\n",
    "scores[\"F1-score\"].append(cv_result[\"test_f1_macro\"].mean())\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Run a pipeline with under-sampling and a random forest classifier.\n",
    "# TODO: replace the content of this cell with your solution\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
